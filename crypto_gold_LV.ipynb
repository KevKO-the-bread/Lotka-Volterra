"""
VFGLVM: Gold ETF Market Cap vs Total Crypto (BTC+ETH)
=====================================================

Two modes:
1. Monthly mode (as before)
   x = Gold ETF AUM (USD), month-end
   y = Total crypto (BTC+ETH) market cap (USD), month-end

2. Daily mode (new)
   - Gold ETF ounces are observed month-end -> interpolated day by day
   - LBMA gold price is daily
   - gold_etf_aum_daily = ounces_daily * gold_price_usd_daily
   - crypto_total_usd is daily BTC+ETH mcap
   -> run VFGLVM on daily data

Also:
- Explicit CV split builder with walk-forward expanding window.
- Same VFGLVM core.
"""

from __future__ import annotations
import math, json
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
from scipy.special import gammaln

# ============================================================================
# CONFIG
# ============================================================================

START_DATE = "2016-01-01"
END_DATE   = "2025-09-30"

PATH_GOLD_ETF   = r"C:\Users\Kevin\Downloads\Thesis\Data\Gold Volume.xlsx"  # cols: Date, Ounces
PATH_LBMA_JSON  = r"C:\Users\Kevin\Downloads\Thesis\Data\LBMA gold price 12.10.2025.json"  # LBMA daily JSON: each rec {'d': date, 'v':[usd,...]}
PATH_CRYPTO_TSV = r"C:\Users\Kevin\Downloads\Thesis\Data\coin-metrics-new-chart.tsv.tsv"   # cols: Time, BTC / Market Cap (USD), ETH / Market Cap (USD)

WINSOR_PCT = 0.005   # light winsorization on crypto caps (helps huge spikes)

OUTDIR_MONTHLY = Path.cwd() / "output" / "vfglvm_gold_vs_crypto_monthly"
OUTDIR_DAILY   = Path.cwd() / "output" / "vfglvm_gold_vs_crypto_daily"
for _p in [OUTDIR_MONTHLY, OUTDIR_DAILY]:
    _p.mkdir(parents=True, exist_ok=True)

mpl.rcParams.update({
    "figure.dpi": 100,
    "axes.titlesize": 12,
    "axes.labelsize": 11,
    "xtick.labelsize": 9,
    "ytick.labelsize": 9,
    "grid.alpha": 0.3,
    "grid.linestyle": ":",
})
plt.style.use("seaborn-v0_8-darkgrid")
sns.set_palette("husl")

# ============================================================================
# TIME INDEX & ALIGN HELPERS
# ============================================================================

def month_end_index(start_date: str, end_date: str) -> pd.DatetimeIndex:
    # month-end frequency
    return pd.date_range(pd.to_datetime(start_date), pd.to_datetime(end_date), freq="ME")

def daily_index(start_date: str, end_date: str) -> pd.DatetimeIndex:
    return pd.date_range(pd.to_datetime(start_date), pd.to_datetime(end_date), freq="D")

def align_level_to_eom(series: pd.Series, eom_idx: pd.DatetimeIndex) -> pd.Series:
    """
    For each month-end timestamp ts in eom_idx, take the last observation
    from 'series' with index <= ts.
    """
    series = series.sort_index()
    out = []
    for ts in eom_idx:
        hist = series.loc[series.index <= ts]
        out.append(hist.iloc[-1] if len(hist) else np.nan)
    return pd.Series(out, index=eom_idx)

# ============================================================================
# CORE VFGLVM IMPLEMENTATION
# ============================================================================

def _as_array(x) -> np.ndarray:
    return np.asarray(x, dtype=float)

def _ridge(X: np.ndarray, y: np.ndarray, alpha: float = 1e-6) -> np.ndarray:
    XtX = X.T @ X
    XtX[np.diag_indices_from(XtX)] += alpha
    Xty = X.T @ y
    return np.linalg.solve(XtX, Xty)

def _rolling_slope(y: np.ndarray, window: int) -> np.ndarray:
    """
    Rolling OLS slope estimate for q(t).
    """
    n = len(y)
    out = np.full(n, np.nan)
    t = np.arange(n, dtype=float)

    for end in range(window - 1, n):
        start = end - window + 1
        yy = y[start:end+1]
        tt = t[start:end+1]

        mask = np.isfinite(yy)
        if mask.sum() < max(2, window // 2):
            continue

        yyv = yy[mask]
        ttv = tt[mask]

        ym = np.mean(yyv)
        tm = np.mean(ttv)

        num = np.sum((ttv - tm) * (yyv - ym))
        den = np.sum((ttv - tm)**2)
        out[end] = np.nan if den == 0 else num / den
    return out

def estimate_q_t(series_matrix: np.ndarray,
                 window: int,
                 clip: Tuple[float,float]=(0.3,0.7),
                 smooth_ma: int=4) -> np.ndarray:
    """
    Estimate variable fractional order q(t):
    1. rolling slope magnitude per series
    2. average across series
    3. scale by 75th percentile
    4. clamp to [0.3,0.7]
    5. smooth moving average
    """
    n, k = series_matrix.shape

    if window > k // 2:
        window = max(4, k // 4)
        print(f"   [WARN] q(t) window cut to {window}")

    slopes_abs = []
    for i in range(n):
        s = _rolling_slope(series_matrix[i], window)
        slopes_abs.append(np.abs(s))

    S = np.nanmean(np.vstack(slopes_abs), axis=0)
    valid = S[np.isfinite(S)]

    if len(valid) < 5:
        q = np.full(k, np.mean(clip))
    else:
        p75 = np.nanpercentile(valid, 75)
        if not np.isfinite(p75) or p75 <= 0:
            q = np.full(k, np.mean(clip))
        else:
            q = S / p75
        q = pd.Series(q).ffill().bfill().values
        q = np.clip(q, clip[0], clip[1])

    if smooth_ma and smooth_ma > 1:
        q = pd.Series(q).rolling(smooth_ma, min_periods=1, center=True).mean().values
        q = np.clip(q, clip[0], clip[1])

    return q

def fractional_accum_variable_order(x: np.ndarray, q_t: np.ndarray, max_lag: int) -> np.ndarray:
    """
    Variable-order fractional accumulation using Gamma ratios.
    """
    n = len(x)
    if len(q_t) != n:
        raise ValueError("q_t must have same length as x")

    out = np.zeros(n, dtype=float)

    for k in range(n):
        qk = float(np.clip(q_t[k], 0.05, 0.95))
        i_start = max(0, k - max_lag + 1)
        acc = 0.0

        for i in range(i_start, k+1):
            dist = k - i
            lg = gammaln(qk + dist) - gammaln(qk) - gammaln(dist + 1.0)
            coeff = math.exp(lg)
            acc += coeff * x[i]

        out[k] = acc

    return out

def build_Z_q(levels: List[np.ndarray], q_t: np.ndarray, max_lag: int) -> Tuple[List[np.ndarray], List[np.ndarray]]:
    """
    Build accumulated series X^[q(t)] and the 'background value' Zq
    which is the mean of consecutive accumulated values.
    """
    Xq = [fractional_accum_variable_order(x, q_t, max_lag=max_lag) for x in levels]
    Zq = []
    for xq in Xq:
        z = np.full_like(xq, np.nan)
        z[1:] = 0.5 * (xq[1:] + xq[:-1])
        Zq.append(z)
    return Xq, Zq

@dataclass
class VFGLVMParams:
    a: np.ndarray
    b: np.ndarray
    C: np.ndarray
    def __repr__(self) -> str:
        with np.printoptions(precision=6, suppress=True):
            return f"VFGLVMParams(\n  a={self.a},\n  b={self.b},\n  C=\n{self.C}\n)"

def fit_vfglvm_params(levels: List[np.ndarray], Zq: List[np.ndarray], ridge_alpha: float = 1e-5) -> VFGLVMParams:
    """
    Fit parameters of the generalized Lotka–Volterra form:
    x_i(k+1) ≈ a_i z_i(k) - b_i z_i(k)^2 - Σ_{j≠i} C_ij z_i(k) z_j(k)
    using ridge-regularized least squares.
    """
    n = len(levels)
    T = min(len(x) for x in levels)

    X0 = np.vstack([_as_array(x) for x in levels])[:, :T]
    Z  = np.vstack([_as_array(z) for z in Zq])[:, :T]

    idx_all = np.arange(1, T-1)
    mask = np.ones(len(idx_all), dtype=bool)
    for i in range(n):
        mask &= np.isfinite(Z[i, idx_all]) & np.isfinite(X0[i, idx_all+1])
    idx = idx_all[mask]

    if len(idx) < 5:
        raise RuntimeError(f"Not enough usable points to fit VFGLVM ({len(idx)})")

    a_hat = np.zeros(n)
    b_hat = np.zeros(n)
    C_hat = np.zeros((n, n))

    for i in range(n):
        zi = Z[i, idx]

        cols = [zi, -(zi**2)]
        for j in range(n):
            if j == i:
                continue
            zj = Z[j, idx]
            cols.append(-(zi * zj))

        Bi = np.column_stack(cols)
        Mi = X0[i, idx+1]

        theta = _ridge(Bi, Mi, alpha=ridge_alpha)

        a_hat[i] = theta[0]
        b_hat[i] = theta[1]

        c_idx = 2
        for j in range(n):
            if j == i:
                continue
            C_hat[i, j] = theta[c_idx]
            c_idx += 1

    return VFGLVMParams(a=a_hat, b=b_hat, C=C_hat)

def vfglvm_fitted_values(params: VFGLVMParams, levels: List[np.ndarray], Zq: List[np.ndarray]) -> List[np.ndarray]:
    """
    In-sample 1-step-ahead fit: predict x(k+1) from data at time k.
    """
    n = len(levels)
    T = len(levels[0])
    fitted = [np.full(T, np.nan) for _ in range(n)]

    for k in range(1, T):
        z_k = np.array([Zq[i][k] for i in range(n)])
        if not np.all(np.isfinite(z_k)):
            continue

        for i in range(n):
            pred = params.a[i]*z_k[i] - params.b[i]*(z_k[i]**2)
            for j in range(n):
                if j != i:
                    pred -= params.C[i,j]*z_k[i]*z_k[j]

            if k+1 < T:
                fitted[i][k+1] = pred

    return fitted

def vfglvm_forecast_recursive(params: VFGLVMParams,
                              levels: List[np.ndarray],
                              q_t: np.ndarray,
                              steps_ahead: int,
                              max_lag: int) -> List[np.ndarray]:
    """
    Recursive multi-step forecast:
    simulate forward, feeding forecasts back in.
    """
    n = len(levels)
    sim = [arr.copy() for arr in levels]

    for _ in range(steps_ahead):
        T_cur = len(sim[0])
        q_cur = q_t[min(T_cur-1, len(q_t)-1)]

        _, Zq_cur = build_Z_q(sim, np.full(T_cur, q_cur), max_lag=max_lag)
        z_last = np.array([Zq_cur[i][-1] for i in range(n)])

        nxt = np.zeros(n)
        for i in range(n):
            pred = params.a[i]*z_last[i] - params.b[i]*(z_last[i]**2)
            for j in range(n):
                if j != i:
                    pred -= params.C[i,j]*z_last[i]*z_last[j]
            nxt[i] = max(0.0, pred)

        for i in range(n):
            sim[i] = np.append(sim[i], nxt[i])

    base_T = len(levels[0])
    return [sim[i][base_T:] for fs, i in zip(sim, range(n))]

def metrics_levels(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if not mask.any():
        return dict(RMSE=np.nan, MAE=np.nan, MAPE=np.nan)

    yt = y_true[mask]
    yp = y_pred[mask]

    err = yt - yp
    rmse = float(np.sqrt(np.mean(err**2)))
    mae  = float(np.mean(np.abs(err)))

    with np.errstate(divide='ignore', invalid='ignore'):
        ape = np.abs(err)/np.abs(yt)
        ape = ape[np.isfinite(ape)]
        mape = float(np.mean(ape)*100) if len(ape)>0 else np.nan

    return dict(RMSE=rmse, MAE=mae, MAPE=mape)

@dataclass
class VFGLVMFit:
    params: VFGLVMParams
    q_t: np.ndarray
    Xq: List[np.ndarray]
    Zq: List[np.ndarray]
    columns: List[str]
    max_lag: int

def _default_max_lag(n_obs: int) -> int:
    # longer memory for long daily / weekly histories
    return 36 if n_obs <= 200 else 156

def build_cv_cutpoints(T: int,
                       min_train: int,
                       val_h: int,
                       n_folds: int) -> List[Tuple[int,int]]:
    """
    Walk-forward expanding-window CV:
    Fold k:
      - Train on [0 : train_end_k)
      - Validate on [train_end_k : train_end_k + val_h)

    train_end_k grows over folds, so we never use future data to predict the past.
    """
    cutpoints: List[Tuple[int,int]] = []
    start = min_train
    for j in range(n_folds):
        train_end = start + j * val_h
        val_end   = train_end + val_h
        if val_end > T:
            break
        cutpoints.append((train_end, val_end))
    return cutpoints

def fit_vfglvm(df: pd.DataFrame,
               window_q: int,
               n_folds: int,
               min_train: int,
               val_h: int,
               smooth_ma_q: int = 4) -> Tuple[VFGLVMFit, pd.DataFrame]:
    """
    Fit VFGLVM to df (2 cols, scaled levels),
    then do walk-forward expanding-window CV using build_cv_cutpoints.
    """
    df = df.dropna().copy().sort_index()
    cols = list(df.columns)
    if len(cols) not in (2,3):
        raise ValueError("Need 2 or 3 columns in df for VFGLVM.")

    levels = [_as_array(df[c].values) for c in cols]
    X0_full = np.vstack(levels)

    print(f"\n[VFGLVM] Estimating q(t), window={window_q} ...")
    q_t = estimate_q_t(X0_full, window=window_q, smooth_ma=smooth_ma_q)
    if not np.isfinite(q_t).any():
        print("   [WARN] q(t) invalid → fallback q=0.5")
        q_t = np.full(len(df), 0.5)
    print(f"   q(t) range: [{np.nanmin(q_t):.3f}, {np.nanmax(q_t):.3f}], mean={np.nanmean(q_t):.3f}")

    max_lag = _default_max_lag(len(df))
    Xq, Zq = build_Z_q(levels, q_t, max_lag=max_lag)

    print("[VFGLVM] Fitting parameters...")
    params = fit_vfglvm_params(levels, Zq, ridge_alpha=1e-5)

    final_fit = VFGLVMFit(
        params=params,
        q_t=q_t,
        Xq=Xq,
        Zq=Zq,
        columns=cols,
        max_lag=max_lag,
    )

    # Walk-forward CV
    T = len(df)
    cp = build_cv_cutpoints(T, min_train=min_train, val_h=val_h, n_folds=n_folds)
    print(f"[CV] {len(cp)} folds using walk-forward expanding window...")

    rows = []
    for fold_id, (tr_end, val_end) in enumerate(cp, start=1):
        train_levels = [s[:tr_end] for s in levels]

        # re-estimate q(t) on train only
        q_tr = estimate_q_t(np.vstack(train_levels), window=window_q, smooth_ma=smooth_ma_q)
        if not np.isfinite(q_tr).any():
            q_tr = np.full(tr_end, 0.5)

        max_lag_tr = _default_max_lag(tr_end)
        Xq_tr, Zq_tr = build_Z_q(train_levels, q_tr, max_lag=max_lag_tr)
        params_tr = fit_vfglvm_params(train_levels, Zq_tr, ridge_alpha=1e-5)

        h_val = val_end - tr_end
        forecasts = vfglvm_forecast_recursive(params_tr, train_levels, q_tr,
                                              steps_ahead=h_val, max_lag=max_lag_tr)

        for i, c in enumerate(cols):
            y_true = levels[i][tr_end:val_end]
            y_pred = forecasts[i]
            m = metrics_levels(y_true, y_pred)
            rows.append({
                "fold": fold_id,
                "series": c,
                "train_n": tr_end,
                "val_n": len(y_true),
                **m
            })
    cv_table = pd.DataFrame(rows)
    return final_fit, cv_table

def fitted_values_df(df: pd.DataFrame, fit: VFGLVMFit) -> pd.DataFrame:
    levels = [_as_array(df[c].values) for c in fit.columns]
    fitted_list = vfglvm_fitted_values(fit.params, levels, fit.Zq)
    return pd.DataFrame(index=df.index,
                        data={col: fitted_list[i] for i, col in enumerate(fit.columns)})

# ============================================================================
# DATA LOADING HELPERS
# ============================================================================

def _winsorize(s: pd.Series, p: float | None) -> pd.Series:
    if p is None or p <= 0:
        return s
    lo = s.quantile(p)
    hi = s.quantile(1 - p)
    return s.clip(lower=lo, upper=hi)

# ---- 1) Gold ETF holdings (monthly ounces, month-end aligned) ----
def load_gold_etf_holdings_monthly(path: str) -> pd.DataFrame:
    print("\n[1/3] Loading Gold ETF Holdings (ounces, month-end)...")
    df = pd.read_excel(path)
    if "Date" not in df.columns or "Ounces" not in df.columns:
        raise ValueError(f"Expected columns 'Date' and 'Ounces'. Found: {list(df.columns)}")
    df = df[["Date", "Ounces"]].copy()
    df["Date"] = pd.to_datetime(df["Date"], errors="coerce")
    df["Ounces"] = pd.to_numeric(df["Ounces"], errors="coerce")
    df = df.dropna().sort_values("Date")

    df = df[(df["Date"] >= pd.to_datetime(START_DATE)) &
            (df["Date"] <= pd.to_datetime(END_DATE))]

    eom_idx = month_end_index(START_DATE, END_DATE)
    s_oz = pd.Series(df["Ounces"].values, index=df["Date"].values)
    s_eom_oz = align_level_to_eom(s_oz, eom_idx)

    out = pd.DataFrame({"gold_etf_ounces": s_eom_oz}, index=eom_idx)
    print(f"   Loaded {out['gold_etf_ounces'].notna().sum()} months | "
          f"{out.index.min().date()} → {out.index.max().date()}")
    return out

# ---- 2a) LBMA gold price DAILY (USD/oz) ----
def load_lbma_gold_price_daily(path: str) -> pd.DataFrame:
    """
    Returns a daily time series of gold price in USD/oz.
    We'll later reindex to full daily range and forward-fill.
    """
    print("\n[LBMA] Loading LBMA Gold Price (USD/oz, DAILY)...")
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)

    rows = []
    for rec in data:
        d = rec.get("d")
        vals = rec.get("v", [])
        usd = vals[0] if isinstance(vals, list) and len(vals) > 0 else None
        rows.append((d, usd))

    df = pd.DataFrame(rows, columns=["date", "usd_per_oz"])
    df["date"] = pd.to_datetime(df["date"], errors="coerce")
    df["usd_per_oz"] = pd.to_numeric(df["usd_per_oz"], errors="coerce")
    df = df.dropna().sort_values("date")

    df = df[(df["date"] >= pd.to_datetime(START_DATE)) &
            (df["date"] <= pd.to_datetime(END_DATE))]

    df = df.set_index("date")

    # Build continuous daily index and ffill price
    full_idx = daily_index(START_DATE, END_DATE)
    df_daily = df.reindex(full_idx).ffill()

    df_daily.columns = ["gold_usd_per_oz"]
    print(f"   LBMA daily rows after fill: {len(df_daily)} days")
    return df_daily

# ---- 2b) LBMA gold price MONTHLY (month-end) ----
def load_lbma_gold_price_monthly(path: str) -> pd.DataFrame:
    """
    Month-end gold price, via LBMA JSON daily -> align to month-end.
    """
    print("\n[2/3] Loading LBMA Gold Price (USD/oz, month-end)...")
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)

    rows = []
    for rec in data:
        d = rec.get("d")
        vals = rec.get("v", [])
        usd = vals[0] if isinstance(vals, list) and len(vals) > 0 else None
        rows.append((d, usd))

    df = pd.DataFrame(rows, columns=["date", "usd_per_oz"])
    df["date"] = pd.to_datetime(df["date"], errors="coerce")
    df["usd_per_oz"] = pd.to_numeric(df["usd_per_oz"], errors="coerce")
    df = df.dropna().sort_values("date")

    df = df[(df["date"] >= pd.to_datetime(START_DATE)) &
            (df["date"] <= pd.to_datetime(END_DATE))]

    eom_idx = month_end_index(START_DATE, END_DATE)
    s_price = pd.Series(df["usd_per_oz"].values, index=df["date"].values)
    s_eom = align_level_to_eom(s_price, eom_idx)

    out = pd.DataFrame({"gold_usd_per_oz": s_eom}, index=eom_idx)
    print(f"   Loaded {out['gold_usd_per_oz'].notna().sum()} months | "
          f"{out.index.min().date()} → {out.index.max().date()}")
    return out

# ---- 3a) Crypto (BTC+ETH) DAILY ----
def load_crypto_daily(path: str) -> pd.DataFrame:
    """
    Return daily BTC and ETH market caps and total.
    We'll also winsorize extremes to stabilize.
    """
    print("\n[CRYPTO] Loading BTC & ETH Market Cap (USD, DAILY)...")
    encodings = ['utf-16','utf-16-le','utf-16-be','utf-8-sig','latin-1']
    df = None
    for enc in encodings:
        try:
            df = pd.read_csv(path, sep="\t", encoding=enc)
            print(f"   Successfully read with encoding: {enc}")
            break
        except (UnicodeDecodeError, UnicodeError):
            continue
    if df is None:
        raise ValueError("Could not read crypto TSV.")

    need = ["Time", "BTC / Market Cap (USD)", "ETH / Market Cap (USD)"]
    for c in need:
        if c not in df.columns:
            raise ValueError(f"Missing '{c}' in crypto TSV.")

    df = df[need].copy()
    df["Time"] = pd.to_datetime(df["Time"], errors="coerce")
    df["BTC / Market Cap (USD)"] = pd.to_numeric(df["BTC / Market Cap (USD)"], errors="coerce")
    df["ETH / Market Cap (USD)"] = pd.to_numeric(df["ETH / Market Cap (USD)"], errors="coerce")

    df = df.dropna(subset=["Time"]).sort_values("Time")
    df = df[(df["Time"] >= pd.to_datetime(START_DATE)) &
            (df["Time"] <= pd.to_datetime(END_DATE))]

    df = df.set_index("Time")

    # resample to daily calendar, last available point each day
    daily = df.resample("D").last()

    if WINSOR_PCT and WINSOR_PCT > 0:
        daily["BTC / Market Cap (USD)"] = _winsorize(daily["BTC / Market Cap (USD)"], WINSOR_PCT)
        daily["ETH / Market Cap (USD)"] = _winsorize(daily["ETH / Market Cap (USD)"], WINSOR_PCT)

    daily["crypto_total_usd"] = (
        daily["BTC / Market Cap (USD)"].fillna(0)
        + daily["ETH / Market Cap (USD)"].fillna(0)
    )

    daily = daily.rename(columns={
        "BTC / Market Cap (USD)": "btc_mcap_usd",
        "ETH / Market Cap (USD)": "eth_mcap_usd"
    })

    print(f"   Loaded {len(daily)} daily rows | "
          f"{daily.index.min().date()} → {daily.index.max().date()}")
    return daily[["btc_mcap_usd","eth_mcap_usd","crypto_total_usd"]]

# ---- 3b) Crypto (BTC+ETH) MONTHLY ----
def load_crypto_monthly(path: str) -> pd.DataFrame:
    print("\n[3/3] Loading BTC & ETH Market Cap (USD, month-end)...")
    encodings = ['utf-16','utf-16-le','utf-16-be','utf-8-sig','latin-1']
    df = None
    for enc in encodings:
        try:
            df = pd.read_csv(path, sep="\t", encoding=enc)
            print(f"   Successfully read with encoding: {enc}")
            break
        except (UnicodeDecodeError, UnicodeError):
            continue
    if df is None:
        raise ValueError("Could not read crypto TSV.")

    need = ["Time", "BTC / Market Cap (USD)", "ETH / Market Cap (USD)"]
    for c in need:
        if c not in df.columns:
            raise ValueError(f"Missing '{c}' in crypto TSV.")

    df = df[need].copy()
    df["Time"] = pd.to_datetime(df["Time"], errors="coerce")
    df["BTC / Market Cap (USD)"] = pd.to_numeric(df["BTC / Market Cap (USD)"], errors="coerce")
    df["ETH / Market Cap (USD)"] = pd.to_numeric(df["ETH / Market Cap (USD)"], errors="coerce")

    df = df.dropna(subset=["Time"]).sort_values("Time")
    df = df[(df["Time"] >= pd.to_datetime(START_DATE)) &
            (df["Time"] <= pd.to_datetime(END_DATE))]
    df = df.set_index("Time")

    monthly = df.resample("ME").last()  # month-end
    if WINSOR_PCT and WINSOR_PCT > 0:
        monthly["BTC / Market Cap (USD)"] = _winsorize(monthly["BTC / Market Cap (USD)"], WINSOR_PCT)
        monthly["ETH / Market Cap (USD)"] = _winsorize(monthly["ETH / Market Cap (USD)"], WINSOR_PCT)
    monthly["crypto_total_usd"] = (
        monthly["BTC / Market Cap (USD)"].fillna(0)
        + monthly["ETH / Market Cap (USD)"].fillna(0)
    )
    monthly = monthly.rename(columns={
        "BTC / Market Cap (USD)": "btc_mcap_usd",
        "ETH / Market Cap (USD)": "eth_mcap_usd"
    })
    print(f"   Loaded {len(monthly)} months | "
          f"{monthly.index.min().date()} → {monthly.index.max().date()}")
    return monthly[["btc_mcap_usd","eth_mcap_usd","crypto_total_usd"]]

# ============================================================================
# DAILY GOLD ETF AUM CONSTRUCTION
# ============================================================================

def interpolate_ounces_daily(gold_ounces_monthly: pd.DataFrame) -> pd.DataFrame:
    """
    Take month-end ETF ounces like:
        2016-01-31 : 100
        2016-02-29 : 130
    and create a DAILY path where ounces increase/decrease *linearly*
    in between those anchors.

    This matches your description: "let the month-to-month ounces increase
    steadily from day to day".
    """
    print("\n[GOLD] Interpolating ETF ounces to DAILY...")
    # gold_ounces_monthly index: month-end
    s_monthly = gold_ounces_monthly["gold_etf_ounces"].copy()
    s_monthly = s_monthly.dropna()

    # build full daily index
    full_idx = daily_index(START_DATE, END_DATE)

    # reindex to full daily index, then 'time' interpolate
    s_daily = s_monthly.reindex(full_idx)
    # linear interpolation in time domain
    s_daily = s_daily.interpolate(method="time")

    df_daily = pd.DataFrame({"gold_etf_ounces_daily": s_daily}, index=full_idx)
    print(f"   Interpolated {df_daily['gold_etf_ounces_daily'].notna().sum()} daily points")
    return df_daily

def build_gold_aum_daily(ounces_daily: pd.DataFrame,
                         gold_price_daily: pd.DataFrame) -> pd.DataFrame:
    """
    ounces_daily: DataFrame with 'gold_etf_ounces_daily' indexed daily
    gold_price_daily: DataFrame with 'gold_usd_per_oz' indexed daily

    Returns DataFrame with column 'gold_etf_mcap_usd' daily.
    """
    df = ounces_daily.join(gold_price_daily, how="inner")
    df["gold_etf_mcap_usd"] = df["gold_etf_ounces_daily"] * df["gold_usd_per_oz"]
    return df[["gold_etf_mcap_usd"]]

# ============================================================================
# PANEL BUILDERS (MONTHLY & DAILY)
# ============================================================================

def build_gold_vs_crypto_panel_monthly(gold_ounces_m: pd.DataFrame,
                                       lbma_price_m: pd.DataFrame,
                                       crypto_m: pd.DataFrame
                                       ) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str,float]]:
    """
    Monthly panel:
    x = gold_etf_mcap_usd (scaled)
    y = crypto_total_usd (scaled)
    """
    print("\n[MERGE] Building Gold ETF vs Total Crypto panel (MONTHLY)...")
    df = gold_ounces_m.join(lbma_price_m, how="outer").join(
        crypto_m[["crypto_total_usd"]], how="outer")
    df = df.sort_index()

    # Gold ETF AUM in USD at month-end
    df["gold_etf_mcap_usd"] = df["gold_etf_ounces"] * df["gold_usd_per_oz"]

    panel_raw = df[["gold_etf_mcap_usd", "crypto_total_usd"]].dropna(how="any").copy()

    # Median scaling (puts values ~O(1) for numerical stability)
    eps = 1e-12
    med_gold  = np.nanmedian(panel_raw["gold_etf_mcap_usd"])
    med_crypto = np.nanmedian(panel_raw["crypto_total_usd"])
    if not (np.isfinite(med_gold) and med_gold > 0):   med_gold = 1.0
    if not (np.isfinite(med_crypto) and med_crypto > 0): med_crypto = 1.0

    panel_scaled = pd.DataFrame({
        "x": panel_raw["gold_etf_mcap_usd"] / (med_gold + eps),
        "y": panel_raw["crypto_total_usd"]   / (med_crypto + eps),
    }, index=panel_raw.index)

    scale_info = {
        "gold_median_usd": med_gold,
        "crypto_total_median_usd": med_crypto
    }

    print(f"   Panel length: {len(panel_scaled)} months | "
          f"{panel_scaled.index.min().date()} → {panel_scaled.index.max().date()}")
    print(f"   Gold ETF median scale:  {med_gold:.3g} USD")
    print(f"   Crypto total median:    {med_crypto:.3g} USD")
    print(f"   Range x: [{panel_scaled['x'].min():.3f}, {panel_scaled['x'].max():.3f}]")
    print(f"   Range y: [{panel_scaled['y'].min():.3f}, {panel_scaled['y'].max():.3f}]")

    return panel_raw, panel_scaled, scale_info

def build_gold_vs_crypto_panel_daily(gold_aum_daily: pd.DataFrame,
                                     crypto_d: pd.DataFrame
                                     ) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str,float]]:
    """
    Daily panel:
    x = gold_etf_mcap_usd (daily)
    y = crypto_total_usd    (daily)
    scaled by medians
    """
    print("\n[MERGE] Building Gold ETF vs Total Crypto panel (DAILY)...")

    df = gold_aum_daily.join(crypto_d[["crypto_total_usd"]], how="inner").sort_index()

    panel_raw = df[["gold_etf_mcap_usd", "crypto_total_usd"]].dropna(how="any").copy()

    eps = 1e-12
    med_gold   = np.nanmedian(panel_raw["gold_etf_mcap_usd"])
    med_crypto = np.nanmedian(panel_raw["crypto_total_usd"])
    if not (np.isfinite(med_gold) and med_gold > 0):   med_gold = 1.0
    if not (np.isfinite(med_crypto) and med_crypto > 0): med_crypto = 1.0

    panel_scaled = pd.DataFrame({
        "x": panel_raw["gold_etf_mcap_usd"] / (med_gold + eps),
        "y": panel_raw["crypto_total_usd"]   / (med_crypto + eps),
    }, index=panel_raw.index)

    scale_info = {
        "gold_median_usd": med_gold,
        "crypto_total_median_usd": med_crypto
    }

    print(f"   Panel length: {len(panel_scaled)} days | "
          f"{panel_scaled.index.min().date()} → {panel_scaled.index.max().date()}")
    print(f"   Gold ETF median scale (daily):   {med_gold:.3g} USD")
    print(f"   Crypto total median (daily):     {med_crypto:.3g} USD")
    print(f"   Range x: [{panel_scaled['x'].min():.3f}, {panel_scaled['x'].max():.3f}]")
    print(f"   Range y: [{panel_scaled['y'].min():.3f}, {panel_scaled['y'].max():.3f}]")

    return panel_raw, panel_scaled, scale_info

# ============================================================================
# RUN PIPELINES (MONTHLY / DAILY)
# ============================================================================

def run_vfglvm_gold_vs_crypto_monthly(outdir: Path):
    """
    Original monthly version.
    """
    gold_ounces_m = load_gold_etf_holdings_monthly(PATH_GOLD_ETF)
    lbma_price_m  = load_lbma_gold_price_monthly(PATH_LBMA_JSON)
    crypto_m      = load_crypto_monthly(PATH_CRYPTO_TSV)

    panel_raw, panel_scaled, scale_info = build_gold_vs_crypto_panel_monthly(
        gold_ounces_m, lbma_price_m, crypto_m
    )

    # Monthly defaults:
    window_q   = 12    # ~1y window for q(t)
    min_train  = 60    # ~5 years before first validation
    val_h      = 12    # forecast 12 months ahead each fold
    n_folds    = 4
    smooth_ma_q = 4

    fit, cv_table = fit_vfglvm(panel_scaled,
                               window_q=window_q,
                               n_folds=n_folds,
                               min_train=min_train,
                               val_h=val_h,
                               smooth_ma_q=smooth_ma_q)

    print("\n[Final VFGLVM Parameters]\n", fit.params)

    # Save panel + CV + fitted values
    outdir.mkdir(parents=True, exist_ok=True)
    panel_raw.to_csv(outdir / "panel_gold_vs_crypto_raw.csv")
    panel_scaled.to_csv(outdir / "panel_gold_vs_crypto_scaled.csv")
    cv_table.to_csv(outdir / "vfglvm_cv_gold_vs_crypto.csv", index=False)

    fitted_df = fitted_values_df(panel_scaled, fit)
    fitted_df.to_csv(outdir / "vfglvm_fitted_gold_vs_crypto_scaled.csv")

    # Plots
    fig, axes = plt.subplots(2,1,figsize=(12,7), sharex=True)
    names = {"x":"Gold ETF Market Cap (scaled)",
             "y":"Total Crypto (BTC+ETH) Market Cap (scaled)"}
    for i, col in enumerate(["x","y"]):
        axes[i].plot(panel_scaled.index, panel_scaled[col],
                     label=f"Actual {names[col]}", lw=2)
        axes[i].plot(panel_scaled.index, fitted_df[col],
                     label=f"VFGLVM Fitted {names[col]}", lw=1.6, alpha=0.85)
        axes[i].grid(alpha=0.3, linestyle=":")
        axes[i].legend()
        axes[i].set_ylabel("Scaled level")
        axes[i].set_title(f"({chr(97+i)}) {names[col]}")
    axes[-1].set_xlabel("Date")
    plt.tight_layout()
    plt.savefig(outdir / "fig_vfglvm_gold_vs_crypto_fitted.png",
                dpi=300, bbox_inches="tight")
    plt.close()

    # q(t)
    fig, ax = plt.subplots(figsize=(12,4))
    ax.plot(panel_scaled.index, fit.q_t, lw=2)
    ax.set_ylabel("q(t)")
    ax.set_xlabel("Date")
    ax.set_title("Variable Fractional Order q(t) Over Time (Monthly)")
    ax.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(outdir / "fig_qt_gold_vs_crypto.png",
                dpi=300, bbox_inches="tight")
    plt.close()

    # CV summary & grade
    cv_summary = cv_table.groupby("series")[["RMSE","MAE","MAPE"]].mean()
    print("\n[CV Mean by Series]")
    print(cv_summary.round(4))
    avg_mape = cv_table["MAPE"].mean()
    if   avg_mape < 10: grade = "✅ HIGHLY ACCURATE (<10%)"
    elif avg_mape < 20: grade = "✅ GOOD (10–20%)"
    elif avg_mape < 50: grade = "⚠️  REASONABLE (20–50%)"
    else:               grade = "❌ POOR (>50%)"
    print(f"\nOverall forecast quality (MAPE): {avg_mape:.1f}% → {grade}")

    # Interpretation
    print("\n[Parameter Interpretation]")
    print(f"Gold intrinsic growth (a[0]):     {fit.params.a[0]:.4f}")
    print(f"Crypto intrinsic growth (a[1]):   {fit.params.a[1]:.4f}")
    print(f"Gold self-limitation (b[0]):      {fit.params.b[0]:.6f}")
    print(f"Crypto self-limitation (b[1]):    {fit.params.b[1]:.6f}")
    print(f"Crypto→Gold interaction C[0,1]:   {fit.params.C[0,1]:.6f}  "
          "(positive = crypto crowds out gold)")
    print(f"Gold→Crypto interaction C[1,0]:   {fit.params.C[1,0]:.6f}  "
          "(positive = gold crowds out crypto)")
    print("\nSign logic: model uses  -C[i,j]*z_i*z_j  →  "
          "positive C = competition (crowding out), negative C = co-movement / support.")

    return {
        "fit": fit,
        "panel_scaled": panel_scaled,
        "panel_raw": panel_raw,
        "cv_table": cv_table,
        "cv_summary": cv_summary,
        "scale_info": scale_info,
    }

def run_vfglvm_gold_vs_crypto_daily(outdir: Path):
    """
    NEW: Daily pipeline with interpolated ETF ounces.
    """
    # 1. Load monthly ounces
    gold_ounces_m = load_gold_etf_holdings_monthly(PATH_GOLD_ETF)

    # 2. Interpolate ounces to daily
    gold_ounces_d = interpolate_ounces_daily(gold_ounces_m)  # -> gold_etf_ounces_daily

    # 3. Daily gold price
    lbma_price_d  = load_lbma_gold_price_daily(PATH_LBMA_JSON)  # -> gold_usd_per_oz (daily)

    # 4. Daily gold ETF AUM
    gold_aum_d = build_gold_aum_daily(gold_ounces_d, lbma_price_d)
    # columns: gold_etf_mcap_usd (daily)

    # 5. Daily crypto (BTC+ETH total)
    crypto_d = load_crypto_daily(PATH_CRYPTO_TSV)
    # columns: btc_mcap_usd, eth_mcap_usd, crypto_total_usd

    # 6. Merge + scale
    panel_raw, panel_scaled, scale_info = build_gold_vs_crypto_panel_daily(
        gold_aum_d, crypto_d
    )

    # Daily defaults:
    # Shorter q-window, shorter forecast horizon, but still walk-forward.
    window_q    = 30    # ~1 month of daily data to estimate q(t)
    min_train   = 365   # ~1 year of history before first validation
    val_h       = 30    # ~1 month ahead forecast horizon
    n_folds     = 4
    smooth_ma_q = 8     # smooth q(t) a bit more for noisy daily

    fit, cv_table = fit_vfglvm(panel_scaled,
                               window_q=window_q,
                               n_folds=n_folds,
                               min_train=min_train,
                               val_h=val_h,
                               smooth_ma_q=smooth_ma_q)

    print("\n[Final VFGLVM Parameters]\n", fit.params)

    # Save
    outdir.mkdir(parents=True, exist_ok=True)
    panel_raw.to_csv(outdir / "panel_gold_vs_crypto_raw_daily.csv")
    panel_scaled.to_csv(outdir / "panel_gold_vs_crypto_scaled_daily.csv")
    cv_table.to_csv(outdir / "vfglvm_cv_gold_vs_crypto_daily.csv", index=False)

    fitted_df = fitted_values_df(panel_scaled, fit)
    fitted_df.to_csv(outdir / "vfglvm_fitted_gold_vs_crypto_scaled_daily.csv")

    # Plots
    fig, axes = plt.subplots(2,1,figsize=(12,7), sharex=True)
    names = {"x":"Gold ETF Market Cap (scaled, daily)",
             "y":"Total Crypto (BTC+ETH) Market Cap (scaled, daily)"}
    for i, col in enumerate(["x","y"]):
        axes[i].plot(panel_scaled.index, panel_scaled[col],
                     label=f"Actual {names[col]}", lw=1.5)
        axes[i].plot(panel_scaled.index, fitted_df[col],
                     label=f"VFGLVM Fitted {names[col]}", lw=1.2, alpha=0.8)
        axes[i].grid(alpha=0.3, linestyle=":")
        axes[i].legend()
        axes[i].set_ylabel("Scaled level")
        axes[i].set_title(f"({chr(97+i)}) {names[col]}")
    axes[-1].set_xlabel("Date")
    plt.tight_layout()
    plt.savefig(outdir / "fig_vfglvm_gold_vs_crypto_fitted_daily.png",
                dpi=300, bbox_inches="tight")
    plt.close()

    fig, ax = plt.subplots(figsize=(12,4))
    ax.plot(panel_scaled.index, fit.q_t, lw=1.5)
    ax.set_ylabel("q(t)")
    ax.set_xlabel("Date")
    ax.set_title("Variable Fractional Order q(t) Over Time (Daily)")
    ax.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(outdir / "fig_qt_gold_vs_crypto_daily.png",
                dpi=300, bbox_inches="tight")
    plt.close()

    # CV summary
    cv_summary = cv_table.groupby("series")[["RMSE","MAE","MAPE"]].mean()
    print("\n[CV Mean by Series] (Daily Forecast Horizon)")
    print(cv_summary.round(4))
    avg_mape = cv_table["MAPE"].mean()
    if   avg_mape < 10: grade = "✅ HIGHLY ACCURATE (<10%)"
    elif avg_mape < 20: grade = "✅ GOOD (10–20%)"
    elif avg_mape < 50: grade = "⚠️  REASONABLE (20–50%)"
    else:               grade = "❌ POOR (>50%)"
    print(f"\nOverall forecast quality (MAPE): {avg_mape:.1f}% → {grade}")

    print("\n[Parameter Interpretation]")
    print(f"Gold intrinsic growth (a[0]):     {fit.params.a[0]:.4f}")
    print(f"Crypto intrinsic growth (a[1]):   {fit.params.a[1]:.4f}")
    print(f"Gold self-limitation (b[0]):      {fit.params.b[0]:.6f}")
    print(f"Crypto self-limitation (b[1]):    {fit.params.b[1]:.6f}")
    print(f"Crypto→Gold interaction C[0,1]:   {fit.params.C[0,1]:.6f}")
    print(f"Gold→Crypto interaction C[1,0]:   {fit.params.C[1,0]:.6f}")
    print("\nSign logic: model uses  -C[i,j]*z_i*z_j  →  "
          "positive C = competition (crowding out), "
          "negative C = risk-on co-movement.")
    return {
        "fit": fit,
        "panel_scaled": panel_scaled,
        "panel_raw": panel_raw,
        "cv_table": cv_table,
        "cv_summary": cv_summary,
        "scale_info": scale_info,
    }

# ============================================================================
# MAIN
# ============================================================================

if __name__ == "__main__":
    print("\n================ MONTHLY RUN ================")
    results_monthly = run_vfglvm_gold_vs_crypto_monthly(OUTDIR_MONTHLY)

    print("\n================= DAILY RUN =================")
    results_daily   = run_vfglvm_gold_vs_crypto_daily(OUTDIR_DAILY)

    print("\nDone.")
